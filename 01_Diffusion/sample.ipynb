{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7xNuCV6XVIM"
   },
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qS6aSM3RP3LL"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oje9wTsOXU1b"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5NfORMRBlkhB"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAL1dpULXe-V"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ohL9QifTXUx5"
   },
   "outputs": [],
   "source": [
    "# train에서 구현한 model 코드 그대로 사용하시면 됩니다\n",
    "\n",
    "def swish(x):\n",
    "    # DDPM에서 쓰이는 활성화함수\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "def get_timestep_embedding(t,channel):\n",
    "    # DDPM은 타임스텝 t 또한 입력으로 받아 이미지에 낀 노이즈의 값을 예측합니다.\n",
    "    # get_timestep_embedding 함수는 batch_size만큼의 타임스텝 t를 입력으로 받아서,\n",
    "    # 이에 대응되는 embedding vector(torch.Size([B,channel]))를 반환합니다.\n",
    "    # Transformer의 sinusodial positional encoding과 동일합니다. (자세한 수식은 PPT 참고해주세요.)\n",
    "\n",
    "    # channel 값은 짝수 int로 입력해야 코드가 잘 작동합니다.\n",
    "    half = channel // 2\n",
    "    device = t.device\n",
    "\n",
    "    # (구현) timestep embedding\n",
    "    inv_freq = torch.exp(-math.log(10000) * torch.arange(half)/half).to(device)\n",
    "    args = t.float().unsqueeze(1) * inv_freq.unsqueeze(0)\n",
    "    emb = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)\n",
    "\n",
    "    return emb\n",
    "\n",
    "class GroupNorm(nn.GroupNorm):\n",
    "    # Group normalization\n",
    "    def __init__(self, num_channels, num_groups=8, eps=1e-6):\n",
    "        super().__init__(num_groups, num_channels, eps=eps)\n",
    "\n",
    "def conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True, init_scale=1.0):\n",
    "    # 3x3 convolution\n",
    "    conv = nn.Conv2d(in_ch, out_ch, kernel_size, stride, padding, bias=bias)\n",
    "    with torch.no_grad():\n",
    "        # init_scale==0으로 설정시 처음에 레이어의 가중치가 0으로 초기화됩니다.\n",
    "        conv.weight.data *= init_scale\n",
    "    return conv\n",
    "\n",
    "def nin(in_ch, out_ch, init_scale=1.0):\n",
    "    # 1x1 convolution\n",
    "    layer = nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=1, padding=0)\n",
    "    with torch.no_grad():\n",
    "        layer.weight.data *= init_scale\n",
    "    return layer\n",
    "\n",
    "def linear(in_features, out_features, init_scale=1.0):\n",
    "    # linear layer\n",
    "    fc = nn.Linear(in_features, out_features)\n",
    "    with torch.no_grad():\n",
    "        fc.weight.data *= init_scale\n",
    "    return fc\n",
    "\n",
    "class DownsampleBlock(nn.Module):\n",
    "    # B*C*2H*2W 이미지를 입력으로 받아, B*C*H*W 이미지를 반환합니다.\n",
    "    # with_conv 여부에 따라 downsampling 방식이 달라집니다.\n",
    "    def __init__(self, channels, with_conv=True):\n",
    "        super().__init__()\n",
    "        if with_conv:\n",
    "            self.op = nn.Conv2d(channels, channels, kernel_size=3, stride=2, padding=1)\n",
    "        else:\n",
    "            self.op = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.op(x)\n",
    "\n",
    "class UpsampleBlock(nn.Module):\n",
    "    # B*C*H*W 이미지를 입력으로 받아, B*C*2H*2W 이미지를 반환합니다.\n",
    "    # with_conv 여부에 따라 upsampling 방식이 달라집니다.\n",
    "    def __init__(self, channels, with_conv=True):\n",
    "        super().__init__()\n",
    "        self.with_conv = with_conv\n",
    "        if with_conv:\n",
    "            self.conv = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor=2.0, mode='nearest')\n",
    "        if self.with_conv:\n",
    "            x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    # DDPM의 주요 구성요소 중 하나인 ResnetBlock입니다.\n",
    "    # 모델 구조 및 구현에 관한 세부사항은 PPT 참고바랍니다.\n",
    "    # 모든 레이어의 init_scale은 1.0으로 설정합니다.\n",
    "    def __init__(self, in_channels, out_channels, temb_channels=256, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.temb_channels = temb_channels # channel of timestep embedding\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # (구현) ResnetBlock\n",
    "        self.norm1 = GroupNorm(self.in_channels)\n",
    "        self.conv1 = conv2d(self.in_channels, self.out_channels,\n",
    "                            kernel_size=3, stride=1, padding=1, init_scale=1.0)\n",
    "        self.temb_proj = linear(self.temb_channels, self.out_channels, init_scale=1.0)\n",
    "        self.norm2 = GroupNorm(self.out_channels)\n",
    "        self.conv2 = conv2d(self.out_channels, self.out_channels,\n",
    "                            kernel_size=3, stride=1, padding=1, init_scale=1.0)\n",
    "        self.conv_shortcut = nin(self.in_channels, self.out_channels)\n",
    "\n",
    "    def forward(self, x, temb):\n",
    "        # B*in_channels*H*W 크기의 텐서 x와 B*temb_channels 크기의 temb를 입력으로 받아서\n",
    "        # B*out_channels*H*W 크기의 텐서를 반환합니다.\n",
    "\n",
    "        # (구현) ResnetBlock\n",
    "        h = self.norm1(x)\n",
    "        h = swish(h)\n",
    "        h = self.conv1(h)\n",
    "        h_temb = swish(temb)\n",
    "        h_temb = self.temb_proj(h_temb)\n",
    "        h_temb = h_temb[:, :, None, None]\n",
    "        h = h + h_temb\n",
    "        h = self.norm2(h)\n",
    "        h = swish(h)\n",
    "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        h = self.conv2(h)\n",
    "        x = self.conv_shortcut(x)\n",
    "        return x + h\n",
    "\n",
    "class AttnBlock(nn.Module):\n",
    "    # DDPM의 주요 구성요소 중 하나인 ResnetBlock입니다.\n",
    "    # 모델 구조 및 구현에 관한 세부사항은 PPT 참고바랍니다.\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.norm = GroupNorm(channels)\n",
    "        self.q = nin(channels, channels)\n",
    "        self.k = nin(channels, channels)\n",
    "        self.v = nin(channels, channels)\n",
    "        self.proj_out = nin(channels, channels, init_scale=0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        h = self.norm(x)\n",
    "\n",
    "        # Q,K,V 텐서 생성\n",
    "        q = self.q(h)\n",
    "        k = self.k(h)\n",
    "        v = self.v(h)\n",
    "\n",
    "        q = q.view(B, C, H * W).permute(0, 2, 1)  # (B, T, C)\n",
    "        k = k.view(B, C, H * W).permute(0, 2, 1)  # (B, T, C)\n",
    "        v = v.view(B, C, H * W).permute(0, 2, 1)  # (B, T, C)\n",
    "\n",
    "        scale = q.shape[-1] ** -0.5\n",
    "\n",
    "        attn = torch.softmax(torch.bmm(q, k.transpose(1, 2)) * scale, dim=-1)\n",
    "        h_ = torch.bmm(attn, v)\n",
    "        h_ = h_.permute(0, 2, 1).reshape(B, C, H, W)\n",
    "\n",
    "        h_ = self.proj_out(h_) # torch.Size([B,C,H,W])\n",
    "        return x + h_\n",
    "\n",
    "class DDPMModel(nn.Module):\n",
    "    # 최종 DDPM 모델입니다.\n",
    "    # Downsample, Middle, Upsample 블락으로 구성됩니다.\n",
    "    # 각 블락은 resnetblock, attentionblock으로 구성됩니다.\n",
    "    # 모델 구조 및 구현에 관한 세부사항은 PPT 참고바랍니다.\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=3,\n",
    "        out_channels=3,\n",
    "        ch=64,\n",
    "        ch_mult=(1,2,4),\n",
    "        num_res_blocks=2,\n",
    "        attn_resolutions={32},\n",
    "        dropout=0.0,\n",
    "        resamp_with_conv=False,\n",
    "        init_resolution=64\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.ch = ch\n",
    "        self.ch_mult = ch_mult\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.attn_resolutions = attn_resolutions\n",
    "        self.dropout = dropout\n",
    "        self.num_levels = len(ch_mult)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.resamp_with_conv = resamp_with_conv\n",
    "        self.init_resolution = init_resolution\n",
    "\n",
    "        # Timestep embedding channel\n",
    "        self.temb_ch = ch * 4\n",
    "\n",
    "        # Timestep embedding layers\n",
    "        self.temb_dense0 = linear(self.ch, self.temb_ch)\n",
    "        self.temb_dense1 = linear(self.temb_ch, self.temb_ch)\n",
    "\n",
    "        # Input conv\n",
    "        self.conv_in = conv2d(in_channels, ch, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # (구현) Downsample blocks\n",
    "        self.down_blocks = nn.ModuleList()\n",
    "\n",
    "        curr_ch = ch\n",
    "        curr_res = init_resolution\n",
    "        for level in range(self.num_levels):\n",
    "            level_blocks = nn.ModuleList()\n",
    "            out_ch = ch * ch_mult[level]\n",
    "            for i in range(num_res_blocks):\n",
    "                level_blocks.append(ResnetBlock(curr_ch, out_ch, temb_channels=self.temb_ch, dropout=dropout))\n",
    "                if curr_res in attn_resolutions:\n",
    "                    level_blocks.append(AttnBlock(out_ch))\n",
    "                curr_ch = out_ch\n",
    "            self.down_blocks.append(level_blocks)\n",
    "            if level != self.num_levels - 1:\n",
    "                self.down_blocks.append(DownsampleBlock(curr_ch, with_conv=resamp_with_conv))\n",
    "                curr_res //= 2\n",
    "\n",
    "        # (구현) Middle blocks\n",
    "        self.mid_block = nn.ModuleList()\n",
    "\n",
    "        self.mid_block = nn.ModuleList([\n",
    "            ResnetBlock(curr_ch, curr_ch, temb_channels=self.temb_ch, dropout=dropout),\n",
    "            AttnBlock(curr_ch),\n",
    "            ResnetBlock(curr_ch, curr_ch, temb_channels=self.temb_ch, dropout=dropout)\n",
    "        ])\n",
    "\n",
    "        # (구현) Upsample blocks\n",
    "        self.up_blocks = nn.ModuleList()\n",
    "\n",
    "        self.up_blocks = nn.ModuleList()\n",
    "        for level in reversed(range(self.num_levels)):\n",
    "            level_blocks = nn.ModuleList()\n",
    "            out_ch = ch * ch_mult[level]\n",
    "            level_blocks.append(ResnetBlock(curr_ch + out_ch, out_ch, temb_channels=self.temb_ch, dropout=dropout))\n",
    "            if (init_resolution // (2 ** level)) in attn_resolutions:\n",
    "                level_blocks.append(AttnBlock(out_ch))\n",
    "            curr_ch = out_ch\n",
    "            for i in range(num_res_blocks):\n",
    "                level_blocks.append(ResnetBlock(curr_ch, curr_ch, temb_channels=self.temb_ch, dropout=dropout))\n",
    "                if (init_resolution // (2 ** level)) in attn_resolutions:\n",
    "                    level_blocks.append(AttnBlock(curr_ch))\n",
    "            if level != 0:\n",
    "                level_blocks.append(UpsampleBlock(curr_ch, with_conv=resamp_with_conv))\n",
    "            self.up_blocks.append(level_blocks)\n",
    "\n",
    "        # Output conv\n",
    "        self.norm_out = GroupNorm(curr_ch)\n",
    "        self.conv_out = conv2d(curr_ch, out_channels, kernel_size=3, stride=1, padding=1, init_scale=0.0)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Timestep embedding\n",
    "        temb = get_timestep_embedding(t, self.ch) # B*ch\n",
    "        temb = self.temb_dense0(temb) # B*4ch\n",
    "        temb = swish(temb)\n",
    "        temb = self.temb_dense1(temb) # B*4ch\n",
    "\n",
    "        skips = []\n",
    "        h = self.conv_in(x)\n",
    "\n",
    "        # (구현) down_blocks에 있는 레이어들을 따라 downsampling 진행\n",
    "        # 각 레이어의 output을 skips 리스트에 저장합니다. (이는 Upsample시 활용됩니다)\n",
    "        # resnetblock과 attentionblock에 들어가는 input이 다름을 주의해주세요.\n",
    "        down_iter = iter(self.down_blocks)\n",
    "        for level in range(self.num_levels):\n",
    "            blocks = next(down_iter)\n",
    "            for layer in blocks:\n",
    "                h = layer(h, temb) if isinstance(layer, ResnetBlock) else layer(h)\n",
    "            skips.append(h)\n",
    "            if level != self.num_levels - 1:\n",
    "                downsample = next(down_iter)\n",
    "                h = downsample(h)\n",
    "\n",
    "        # (구현) mid_blocks에 있는 레이어들을 따라 진행\n",
    "        for layer in self.mid_block:\n",
    "            h = layer(h, temb) if isinstance(layer, ResnetBlock) else layer(h)\n",
    "\n",
    "        # (구현) up_blocks에 있는 레이어들을 따라 upsampling 진행\n",
    "        # downsample 과정에서 구한 텐서와 병합 후 레이어에 넣어줍니다.\n",
    "        for level in range(self.num_levels):\n",
    "            blocks = self.up_blocks[level]\n",
    "            skip = skips.pop()\n",
    "            h = torch.cat([h, skip], dim=1)\n",
    "            h = blocks[0](h, temb)\n",
    "            for layer in blocks[1:]:\n",
    "                if isinstance(layer, ResnetBlock):\n",
    "                    h = layer(h, temb)\n",
    "                else:\n",
    "                    h = layer(h)\n",
    "\n",
    "        # Output\n",
    "        h = self.norm_out(h)\n",
    "        h = swish(h)\n",
    "        h = self.conv_out(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8fIxWYeYTfN"
   },
   "source": [
    "# Get alphas_cumprod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "deIcETNuYWA-"
   },
   "outputs": [],
   "source": [
    "def get_beta_alpha_linear(beta_start=0.0001, beta_end=0.02, num_timesteps=1000):\n",
    "    # DDPM 학습 및 샘플링에 쓰일 alpha, beta, alphas_cumprod 반환\n",
    "    # Train에서 쓰인 함수와 정확히 같습니다.\n",
    "\n",
    "    betas = np.linspace(beta_start, beta_end, num_timesteps, dtype=np.float32)\n",
    "    betas = torch.tensor(betas)\n",
    "    alphas = 1.0 - betas\n",
    "    alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "    return betas, alphas, alphas_cumprod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BS93rvu-Xtzd"
   },
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rk3zMv1RXUu_"
   },
   "outputs": [],
   "source": [
    "def p_sample_ddim(model, x_t, t_cur, t_prev, alphas_cumprod, eta=0.0):\n",
    "    # DDIM reverse process의 단일 스텝 코드입니다. 이전 스텝의 이미지 x_t를 받아서 다음 스텝의 이미지 x_prev를 반환합니다.\n",
    "    # Ex) 480(t_cur) 타임스텝에서의 이미지를 입력으로 받아 475(t_prev) 타임스텝의 이미지를 반환\n",
    "\n",
    "    # 이전스텝의 alpha_bar\n",
    "    alpha_bar_t = alphas_cumprod[t_cur-1]\n",
    "    # 다음스텝의 alpha_bar\n",
    "    if t_prev > 0:\n",
    "        alpha_bar_prev = alphas_cumprod[t_prev-1]\n",
    "    else:\n",
    "        alpha_bar_prev = torch.tensor(1.0, device=x_t.device)\n",
    "\n",
    "    # (구현) eta, alpha_bar_t, alpha_bar_prev 이용해서 DDIM의 sigma_t 구현\n",
    "    sigma_t = eta * torch.sqrt((1 - alpha_bar_prev) / (1 - alpha_bar_t)) \\\n",
    "              * torch.sqrt(1 - alpha_bar_t / alpha_bar_prev)\n",
    "\n",
    "    # (구현) 학습된 모델을 이용해 노이즈 레벨 eps_theta 예측\n",
    "    B = x_t.size(0)\n",
    "    t_tensor = torch.full((B,), t_cur, device=x_t.device, dtype=torch.long)\n",
    "    eps_theta = model(x_t, t_tensor)\n",
    "\n",
    "    # (구현) x_t, eps_theta 이용해 x0 예측\n",
    "    sqrt_ab_t    = torch.sqrt(alpha_bar_t)\n",
    "    sqrt_ab_prev = torch.sqrt(alpha_bar_prev)\n",
    "    x0_pred = (x_t - torch.sqrt(1 - alpha_bar_t).view(-1,1,1,1) * eps_theta) \\\n",
    "              / sqrt_ab_t.view(-1,1,1,1)\n",
    "\n",
    "    # DDIM의 방향성 컴포넌트 구현\n",
    "    dir_xt = torch.sqrt(torch.clamp(1 - alpha_bar_prev - sigma_t**2, min=0.0)).view(-1,1,1,1) * eps_theta\n",
    "\n",
    "    # 노이즈 추가\n",
    "    noise = torch.randn_like(x_t) if t_prev > 0 else torch.zeros_like(x_t)\n",
    "\n",
    "    # (구현) x0_pred, dir_xt, simga_t, noise 이용해서 다음스텝의 이미지 계산\n",
    "    x_prev = sqrt_ab_prev.view(-1,1,1,1) * x0_pred + dir_xt + sigma_t.view(-1,1,1,1) * noise\n",
    "\n",
    "    return x_prev\n",
    "\n",
    "def sample_ddim(model, shape, alphas_cumprod, device, ddim_steps, eta=0.0):\n",
    "    # p_sample_ddim을 적절한 timestep을 따라서 반복하는 최종 DDIM 샘플링 함수\n",
    "\n",
    "    num_timesteps = alphas_cumprod.shape[0]\n",
    "    x = torch.randn(shape, device=device)\n",
    "\n",
    "    # 0-based 인덱스로 균등 서브샘플링\n",
    "    idx_lin = torch.linspace(0, num_timesteps-1, steps=ddim_steps+1, device=device)\n",
    "    idx0 = idx_lin.round().long()\n",
    "\n",
    "    # 항상 처음(0)과 끝(T-1) 포함하여 중복 제거 & 정렬\n",
    "    idx0 = torch.cat([torch.tensor([0, num_timesteps-1], device=device, dtype=torch.long),idx0]).unique(sorted=True)\n",
    "\n",
    "    # 1-based timestep으로 변환\n",
    "    seq_asc = idx0 + 1 # Ex) [1,5,10,...,995,1000]\n",
    "    # 역순으로 뒤집기\n",
    "    seq_rev = torch.flip(seq_asc, dims=[0])\n",
    "    # 마지막에 0 추가\n",
    "    seq = torch.cat([seq_rev, torch.tensor([0], device=device, dtype=torch.long)]) # Ex) [1000,995,...,5,1,0]\n",
    "\n",
    "    # seq를 따라서 p_sample_ddim 반복적으로 수행\n",
    "    prev_t = seq[0].item() # Ex) prev_t = 1000\n",
    "    for next_t in tqdm(seq[1:],desc='Sampling'):\n",
    "        t_cur  = prev_t\n",
    "        t_prev = next_t.item()\n",
    "\n",
    "        x = p_sample_ddim(\n",
    "            model,\n",
    "            x,                   # 현재 노이즈 x_{t_cur}\n",
    "            t_cur,               # 현재 스텝 (마지막엔 1)\n",
    "            t_prev,              # 다음 스텝 (마지막엔 0)\n",
    "            alphas_cumprod,\n",
    "            eta\n",
    "        )\n",
    "        prev_t = t_prev # 다음 반복의 \"현재\"가 됩니다\n",
    "\n",
    "    return x.cpu().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0T3YfdR3XUsG"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "_, _, alphas_cumprod = get_beta_alpha_linear()\n",
    "alphas_cumprod = alphas_cumprod.to(device)\n",
    "\n",
    "# 학습할 때와 같은 batch_size 사용 권장\n",
    "batch_size = 64\n",
    "image_size = 64\n",
    "\n",
    "# train할 때 저장한 모델 파라미터 pt파일 불러오기\n",
    "model = DDPMModel().to(device)\n",
    "model_path = ''\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f'Model is loaded')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    x0 = sample_ddim(model,\n",
    "                     shape=(batch_size, 3, image_size, image_size),\n",
    "                     alphas_cumprod=alphas_cumprod,\n",
    "                     ddim_steps=200,\n",
    "                     device=device\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wviWbKLdYqf3"
   },
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CfptlxNtXUpW"
   },
   "outputs": [],
   "source": [
    "def visualize_sample(x0, idx=0):\n",
    "    img = x0[idx]\n",
    "    img = np.transpose(img, (1, 2, 0))\n",
    "    img = (img + 1.0) / 2.0\n",
    "    img = np.clip(img, 0, 1)\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Sampled x_0\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ubg3evx5YsKi"
   },
   "outputs": [],
   "source": [
    "visualize_sample(x0)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN0323jS2l8PVyLsthqQM+a",
   "collapsed_sections": [
    "m7xNuCV6XVIM",
    "wAL1dpULXe-V",
    "u8fIxWYeYTfN"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
