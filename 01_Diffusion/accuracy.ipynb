{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ev1z4SkiWdAT"
   },
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cSN_Ln78-oSH"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "-rH6JIumBwXL"
   },
   "outputs": [],
   "source": [
    "!pip install lpips\n",
    "import lpips\n",
    "\n",
    "!pip install pytorch-fid\n",
    "from pytorch_fid import fid_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lNXm8TCsVVrN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LlcrPYwgj_on"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JrEllYdniMPW"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "kD9rs9M1R6Aj"
   },
   "outputs": [],
   "source": [
    "class CelebADataset(Dataset):\n",
    "    def __init__(self, img_dir, img_num_list, transform=None):\n",
    "        self.img_paths = [os.path.join(img_dir, f'{i:06d}.jpg') for i in img_num_list]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.img_paths[idx]\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FI9I5xVcQ_ZJ"
   },
   "outputs": [],
   "source": [
    "# CelebA 압축파일 Colab 런타임 VM으로 복사\n",
    "# 디렉토리에 celeba_dataset.zip 파일이 있어야 합니다.\n",
    "!cp /content/sample_directory/celeba_dataset.zip /content/\n",
    "\n",
    "# VM에 파일 압축풀기\n",
    "# /content/data 경로에 모든 이미지가 저장됩니다.\n",
    "!unzip -q /content/celeba_dataset.zip -d /content/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a8qc0dCMkN5I"
   },
   "outputs": [],
   "source": [
    "num_data = 30000\n",
    "train_test_ratio = 0.9\n",
    "batch_size = 64\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize([64,64]),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize([64,64]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "])\n",
    "\n",
    "train_img_num = np.arange(1,num_data+1)[:int(num_data*train_test_ratio)]\n",
    "test_img_num = np.arange(1,num_data+1)[int(num_data*train_test_ratio):]\n",
    "\n",
    "train_dataset = CelebADataset('/content/data/img_align_celeba', img_num_list=train_img_num, transform=train_transform)\n",
    "test_dataset = CelebADataset('/content/data/img_align_celeba', img_num_list=test_img_num, transform=test_transform)\n",
    "print(f'Train dataset size: {len(train_dataset)}') # Ex) 27000\n",
    "print(f'Test dataset size: {len(test_dataset)}') # Ex) 3000\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5xNoEOZDYwy"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vom_76xFCzJU"
   },
   "outputs": [],
   "source": [
    "# train에서 구현한 model 코드 그대로 사용하시면 됩니다.\n",
    "\n",
    "def swish(x):\n",
    "    # DDPM에서 쓰이는 활성화함수\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "def get_timestep_embedding(t,channel):\n",
    "    # DDPM은 타임스텝 t 또한 입력으로 받아 이미지에 낀 노이즈의 값을 예측합니다.\n",
    "    # get_timestep_embedding 함수는 batch_size만큼의 타임스텝 t를 입력으로 받아서,\n",
    "    # 이에 대응되는 embedding vector(torch.Size([B,channel]))를 반환합니다.\n",
    "    # Transformer의 sinusodial positional encoding과 동일합니다. (자세한 수식은 PPT 참고해주세요.)\n",
    "\n",
    "    # channel 값은 짝수 int로 입력해야 코드가 잘 작동합니다.\n",
    "    half = channel // 2\n",
    "    device = t.device\n",
    "\n",
    "    # (구현) timestep embedding\n",
    "    inv_freq = torch.exp(-math.log(10000) * torch.arange(half)/half).to(device)\n",
    "    args = t.float().unsqueeze(1) * inv_freq.unsqueeze(0)\n",
    "    emb = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)\n",
    "\n",
    "    return emb\n",
    "\n",
    "class GroupNorm(nn.GroupNorm):\n",
    "    # Group normalization\n",
    "    def __init__(self, num_channels, num_groups=8, eps=1e-6):\n",
    "        super().__init__(num_groups, num_channels, eps=eps)\n",
    "\n",
    "def conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True, init_scale=1.0):\n",
    "    # 3x3 convolution\n",
    "    conv = nn.Conv2d(in_ch, out_ch, kernel_size, stride, padding, bias=bias)\n",
    "    with torch.no_grad():\n",
    "        # init_scale==0으로 설정시 처음에 레이어의 가중치가 0으로 초기화됩니다.\n",
    "        conv.weight.data *= init_scale\n",
    "    return conv\n",
    "\n",
    "def nin(in_ch, out_ch, init_scale=1.0):\n",
    "    # 1x1 convolution\n",
    "    layer = nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=1, padding=0)\n",
    "    with torch.no_grad():\n",
    "        layer.weight.data *= init_scale\n",
    "    return layer\n",
    "\n",
    "def linear(in_features, out_features, init_scale=1.0):\n",
    "    # linear layer\n",
    "    fc = nn.Linear(in_features, out_features)\n",
    "    with torch.no_grad():\n",
    "        fc.weight.data *= init_scale\n",
    "    return fc\n",
    "\n",
    "class DownsampleBlock(nn.Module):\n",
    "    # B*C*2H*2W 이미지를 입력으로 받아, B*C*H*W 이미지를 반환합니다.\n",
    "    # with_conv 여부에 따라 downsampling 방식이 달라집니다.\n",
    "    def __init__(self, channels, with_conv=True):\n",
    "        super().__init__()\n",
    "        if with_conv:\n",
    "            self.op = nn.Conv2d(channels, channels, kernel_size=3, stride=2, padding=1)\n",
    "        else:\n",
    "            self.op = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.op(x)\n",
    "\n",
    "class UpsampleBlock(nn.Module):\n",
    "    # B*C*H*W 이미지를 입력으로 받아, B*C*2H*2W 이미지를 반환합니다.\n",
    "    # with_conv 여부에 따라 upsampling 방식이 달라집니다.\n",
    "    def __init__(self, channels, with_conv=True):\n",
    "        super().__init__()\n",
    "        self.with_conv = with_conv\n",
    "        if with_conv:\n",
    "            self.conv = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor=2.0, mode='nearest')\n",
    "        if self.with_conv:\n",
    "            x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    # DDPM의 주요 구성요소 중 하나인 ResnetBlock입니다.\n",
    "    # 모델 구조 및 구현에 관한 세부사항은 PPT 참고바랍니다.\n",
    "    # 모든 레이어의 init_scale은 1.0으로 설정합니다.\n",
    "    def __init__(self, in_channels, out_channels, temb_channels=256, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.temb_channels = temb_channels # channel of timestep embedding\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # (구현) ResnetBlock\n",
    "        self.norm1 = GroupNorm(self.in_channels)\n",
    "        self.conv1 = conv2d(self.in_channels, self.out_channels,\n",
    "                            kernel_size=3, stride=1, padding=1, init_scale=1.0)\n",
    "        self.temb_proj = linear(self.temb_channels, self.out_channels, init_scale=1.0)\n",
    "        self.norm2 = GroupNorm(self.out_channels)\n",
    "        self.conv2 = conv2d(self.out_channels, self.out_channels,\n",
    "                            kernel_size=3, stride=1, padding=1, init_scale=1.0)\n",
    "        self.conv_shortcut = nin(self.in_channels, self.out_channels)\n",
    "\n",
    "    def forward(self, x, temb):\n",
    "        # B*in_channels*H*W 크기의 텐서 x와 B*temb_channels 크기의 temb를 입력으로 받아서\n",
    "        # B*out_channels*H*W 크기의 텐서를 반환합니다.\n",
    "\n",
    "        # (구현) ResnetBlock\n",
    "        h = self.norm1(x)\n",
    "        h = swish(h)\n",
    "        h = self.conv1(h)\n",
    "        h_temb = swish(temb)\n",
    "        h_temb = self.temb_proj(h_temb)\n",
    "        h_temb = h_temb[:, :, None, None]\n",
    "        h = h + h_temb\n",
    "        h = self.norm2(h)\n",
    "        h = swish(h)\n",
    "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        h = self.conv2(h)\n",
    "        x = self.conv_shortcut(x)\n",
    "        return x + h\n",
    "\n",
    "class AttnBlock(nn.Module):\n",
    "    # DDPM의 주요 구성요소 중 하나인 ResnetBlock입니다.\n",
    "    # 모델 구조 및 구현에 관한 세부사항은 PPT 참고바랍니다.\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.norm = GroupNorm(channels)\n",
    "        self.q = nin(channels, channels)\n",
    "        self.k = nin(channels, channels)\n",
    "        self.v = nin(channels, channels)\n",
    "        self.proj_out = nin(channels, channels, init_scale=0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        h = self.norm(x)\n",
    "\n",
    "        # Q,K,V 텐서 생성\n",
    "        q = self.q(h)\n",
    "        k = self.k(h)\n",
    "        v = self.v(h)\n",
    "\n",
    "        q = q.view(B, C, H * W).permute(0, 2, 1)  # (B, T, C)\n",
    "        k = k.view(B, C, H * W).permute(0, 2, 1)  # (B, T, C)\n",
    "        v = v.view(B, C, H * W).permute(0, 2, 1)  # (B, T, C)\n",
    "\n",
    "        scale = q.shape[-1] ** -0.5\n",
    "\n",
    "        attn = torch.softmax(torch.bmm(q, k.transpose(1, 2)) * scale, dim=-1)\n",
    "        h_ = torch.bmm(attn, v)\n",
    "        h_ = h_.permute(0, 2, 1).reshape(B, C, H, W)\n",
    "\n",
    "        h_ = self.proj_out(h_) # torch.Size([B,C,H,W])\n",
    "        return x + h_\n",
    "\n",
    "class DDPMModel(nn.Module):\n",
    "    # 최종 DDPM 모델입니다.\n",
    "    # Downsample, Middle, Upsample 블락으로 구성됩니다.\n",
    "    # 각 블락은 resnetblock, attentionblock으로 구성됩니다.\n",
    "    # 모델 구조 및 구현에 관한 세부사항은 PPT 참고바랍니다.\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=3,\n",
    "        out_channels=3,\n",
    "        ch=64,\n",
    "        ch_mult=(1,2,4),\n",
    "        num_res_blocks=2,\n",
    "        attn_resolutions={32},\n",
    "        dropout=0.0,\n",
    "        resamp_with_conv=False,\n",
    "        init_resolution=64\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.ch = ch\n",
    "        self.ch_mult = ch_mult\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.attn_resolutions = attn_resolutions\n",
    "        self.dropout = dropout\n",
    "        self.num_levels = len(ch_mult)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.resamp_with_conv = resamp_with_conv\n",
    "        self.init_resolution = init_resolution\n",
    "\n",
    "        # Timestep embedding channel\n",
    "        self.temb_ch = ch * 4\n",
    "\n",
    "        # Timestep embedding layers\n",
    "        self.temb_dense0 = linear(self.ch, self.temb_ch)\n",
    "        self.temb_dense1 = linear(self.temb_ch, self.temb_ch)\n",
    "\n",
    "        # Input conv\n",
    "        self.conv_in = conv2d(in_channels, ch, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # (구현) Downsample blocks\n",
    "        self.down_blocks = nn.ModuleList()\n",
    "\n",
    "        curr_ch = ch\n",
    "        curr_res = init_resolution\n",
    "        for level in range(self.num_levels):\n",
    "            level_blocks = nn.ModuleList()\n",
    "            out_ch = ch * ch_mult[level]\n",
    "            for i in range(num_res_blocks):\n",
    "                level_blocks.append(ResnetBlock(curr_ch, out_ch, temb_channels=self.temb_ch, dropout=dropout))\n",
    "                if curr_res in attn_resolutions:\n",
    "                    level_blocks.append(AttnBlock(out_ch))\n",
    "                curr_ch = out_ch\n",
    "            self.down_blocks.append(level_blocks)\n",
    "            if level != self.num_levels - 1:\n",
    "                self.down_blocks.append(DownsampleBlock(curr_ch, with_conv=resamp_with_conv))\n",
    "                curr_res //= 2\n",
    "\n",
    "        # (구현) Middle blocks\n",
    "        self.mid_block = nn.ModuleList()\n",
    "\n",
    "        self.mid_block = nn.ModuleList([\n",
    "            ResnetBlock(curr_ch, curr_ch, temb_channels=self.temb_ch, dropout=dropout),\n",
    "            AttnBlock(curr_ch),\n",
    "            ResnetBlock(curr_ch, curr_ch, temb_channels=self.temb_ch, dropout=dropout)\n",
    "        ])\n",
    "\n",
    "        # (구현) Upsample blocks\n",
    "        self.up_blocks = nn.ModuleList()\n",
    "\n",
    "        self.up_blocks = nn.ModuleList()\n",
    "        for level in reversed(range(self.num_levels)):\n",
    "            level_blocks = nn.ModuleList()\n",
    "            out_ch = ch * ch_mult[level]\n",
    "            level_blocks.append(ResnetBlock(curr_ch + out_ch, out_ch, temb_channels=self.temb_ch, dropout=dropout))\n",
    "            if (init_resolution // (2 ** level)) in attn_resolutions:\n",
    "                level_blocks.append(AttnBlock(out_ch))\n",
    "            curr_ch = out_ch\n",
    "            for i in range(num_res_blocks):\n",
    "                level_blocks.append(ResnetBlock(curr_ch, curr_ch, temb_channels=self.temb_ch, dropout=dropout))\n",
    "                if (init_resolution // (2 ** level)) in attn_resolutions:\n",
    "                    level_blocks.append(AttnBlock(curr_ch))\n",
    "            if level != 0:\n",
    "                level_blocks.append(UpsampleBlock(curr_ch, with_conv=resamp_with_conv))\n",
    "            self.up_blocks.append(level_blocks)\n",
    "\n",
    "        # Output conv\n",
    "        self.norm_out = GroupNorm(curr_ch)\n",
    "        self.conv_out = conv2d(curr_ch, out_channels, kernel_size=3, stride=1, padding=1, init_scale=0.0)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Timestep embedding\n",
    "        temb = get_timestep_embedding(t, self.ch) # B*ch\n",
    "        temb = self.temb_dense0(temb) # B*4ch\n",
    "        temb = swish(temb)\n",
    "        temb = self.temb_dense1(temb) # B*4ch\n",
    "\n",
    "        skips = []\n",
    "        h = self.conv_in(x)\n",
    "\n",
    "        # (구현) down_blocks에 있는 레이어들을 따라 downsampling 진행\n",
    "        # 각 레이어의 output을 skips 리스트에 저장합니다. (이는 Upsample시 활용됩니다)\n",
    "        # resnetblock과 attentionblock에 들어가는 input이 다름을 주의해주세요.\n",
    "        down_iter = iter(self.down_blocks)\n",
    "        for level in range(self.num_levels):\n",
    "            blocks = next(down_iter)\n",
    "            for layer in blocks:\n",
    "                h = layer(h, temb) if isinstance(layer, ResnetBlock) else layer(h)\n",
    "            skips.append(h)\n",
    "            if level != self.num_levels - 1:\n",
    "                downsample = next(down_iter)\n",
    "                h = downsample(h)\n",
    "\n",
    "        # (구현) mid_blocks에 있는 레이어들을 따라 진행\n",
    "        for layer in self.mid_block:\n",
    "            h = layer(h, temb) if isinstance(layer, ResnetBlock) else layer(h)\n",
    "\n",
    "        # (구현) up_blocks에 있는 레이어들을 따라 upsampling 진행\n",
    "        # downsample 과정에서 구한 텐서와 병합 후 레이어에 넣어줍니다.\n",
    "        for level in range(self.num_levels):\n",
    "            blocks = self.up_blocks[level]\n",
    "            skip = skips.pop()\n",
    "            h = torch.cat([h, skip], dim=1)\n",
    "            h = blocks[0](h, temb)\n",
    "            for layer in blocks[1:]:\n",
    "                if isinstance(layer, ResnetBlock):\n",
    "                    h = layer(h, temb)\n",
    "                else:\n",
    "                    h = layer(h)\n",
    "\n",
    "        # Output\n",
    "        h = self.norm_out(h)\n",
    "        h = swish(h)\n",
    "        h = self.conv_out(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JlSYB0-wDf38"
   },
   "source": [
    "# Get alphas_cumprod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uyd4Ac76DcLQ"
   },
   "outputs": [],
   "source": [
    "def get_beta_alpha_linear(beta_start=0.0001, beta_end=0.02, num_timesteps=1000):\n",
    "    # DDPM 학습 및 샘플링에 쓰일 alpha, beta, alphas_cumprod 반환\n",
    "    # Train에서 쓰인 함수와 정확히 같습니다.\n",
    "\n",
    "    betas = np.linspace(beta_start, beta_end, num_timesteps, dtype=np.float32)\n",
    "    betas = torch.tensor(betas)\n",
    "    alphas = 1.0 - betas\n",
    "    alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "    return betas, alphas, alphas_cumprod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pe5Inl-h_xmu"
   },
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nghHkQvZ_vu_"
   },
   "outputs": [],
   "source": [
    "# sample에서 구현한 sampling 코드 그대로 사용하시면 됩니다.\n",
    "\n",
    "def p_sample_ddim(model, x_t, t_cur, t_prev, alphas_cumprod, eta=0.0):\n",
    "    # DDIM reverse process의 단일 스텝 코드입니다. 이전 스텝의 이미지 x_t를 받아서 다음 스텝의 이미지 x_prev를 반환합니다.\n",
    "    # Ex) 480(t_cur) 타임스텝에서의 이미지를 입력으로 받아 475(t_prev) 타임스텝의 이미지를 반환\n",
    "\n",
    "    # 이전스텝의 alpha_bar\n",
    "    alpha_bar_t = alphas_cumprod[t_cur-1]\n",
    "    # 다음스텝의 alpha_bar\n",
    "    if t_prev > 0:\n",
    "        alpha_bar_prev = alphas_cumprod[t_prev-1]\n",
    "    else:\n",
    "        alpha_bar_prev = torch.tensor(1.0, device=x_t.device)\n",
    "\n",
    "    # (구현) eta, alpha_bar_t, alpha_bar_prev 이용해서 DDIM의 sigma_t 구현\n",
    "    sigma_t = eta * torch.sqrt((1 - alpha_bar_prev) / (1 - alpha_bar_t)) \\\n",
    "              * torch.sqrt(1 - alpha_bar_t / alpha_bar_prev)\n",
    "\n",
    "    # (구현) 학습된 모델을 이용해 노이즈 레벨 eps_theta 예측\n",
    "    B = x_t.size(0)\n",
    "    t_tensor = torch.full((B,), t_cur, device=x_t.device, dtype=torch.long)\n",
    "    eps_theta = model(x_t, t_tensor)\n",
    "\n",
    "    # (구현) x_t, eps_theta 이용해 x0 예측\n",
    "    sqrt_ab_t    = torch.sqrt(alpha_bar_t)\n",
    "    sqrt_ab_prev = torch.sqrt(alpha_bar_prev)\n",
    "    x0_pred = (x_t - torch.sqrt(1 - alpha_bar_t).view(-1,1,1,1) * eps_theta) \\\n",
    "              / sqrt_ab_t.view(-1,1,1,1)\n",
    "\n",
    "    # DDIM의 방향성 컴포넌트 구현\n",
    "    dir_xt = torch.sqrt(torch.clamp(1 - alpha_bar_prev - sigma_t**2, min=0.0)).view(-1,1,1,1) * eps_theta\n",
    "\n",
    "    # 노이즈 추가\n",
    "    noise = torch.randn_like(x_t) if t_prev > 0 else torch.zeros_like(x_t)\n",
    "\n",
    "    # (구현) x0_pred, dir_xt, simga_t, noise 이용해서 다음스텝의 이미지 계산\n",
    "    x_prev = sqrt_ab_prev.view(-1,1,1,1) * x0_pred + dir_xt + sigma_t.view(-1,1,1,1) * noise\n",
    "\n",
    "    return x_prev\n",
    "\n",
    "def sample_ddim(model, shape, alphas_cumprod, device, ddim_steps, eta=0.0):\n",
    "    # p_sample_ddim을 적절한 timestep을 따라서 반복하는 최종 DDIM 샘플링 함수\n",
    "\n",
    "    num_timesteps = alphas_cumprod.shape[0]\n",
    "    x = torch.randn(shape, device=device)\n",
    "\n",
    "    # 0-based 인덱스로 균등 서브샘플링\n",
    "    idx_lin = torch.linspace(0, num_timesteps-1, steps=ddim_steps+1, device=device)\n",
    "    idx0 = idx_lin.round().long()\n",
    "\n",
    "    # 항상 처음(0)과 끝(T-1) 포함하여 중복 제거 & 정렬\n",
    "    idx0 = torch.cat([torch.tensor([0, num_timesteps-1], device=device, dtype=torch.long),idx0]).unique(sorted=True)\n",
    "\n",
    "    # 1-based timestep으로 변환\n",
    "    seq_asc = idx0 + 1 # Ex) [1,5,10,...,995,1000]\n",
    "    # 역순으로 뒤집기\n",
    "    seq_rev = torch.flip(seq_asc, dims=[0])\n",
    "    # 마지막에 0 추가\n",
    "    seq = torch.cat([seq_rev, torch.tensor([0], device=device, dtype=torch.long)]) # Ex) [1000,995,...,5,1,0]\n",
    "\n",
    "    # seq를 따라서 p_sample_ddim 반복적으로 수행\n",
    "    prev_t = seq[0].item() # Ex) prev_t = 1000\n",
    "    for next_t in tqdm(seq[1:],desc='Sampling'):\n",
    "        t_cur  = prev_t\n",
    "        t_prev = next_t.item()\n",
    "\n",
    "        x = p_sample_ddim(\n",
    "            model,\n",
    "            x,                   # 현재 노이즈 x_{t_cur}\n",
    "            t_cur,               # 현재 스텝 (마지막엔 1)\n",
    "            t_prev,              # 다음 스텝 (마지막엔 0)\n",
    "            alphas_cumprod,\n",
    "            eta\n",
    "        )\n",
    "        prev_t = t_prev # 다음 반복의 \"현재\"가 됩니다\n",
    "\n",
    "    return x.cpu().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SAq3k8aXjnZN"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "_, _, alphas_cumprod = get_beta_alpha_linear()\n",
    "alphas_cumprod = alphas_cumprod.to(device)\n",
    "\n",
    "# 학습할 때와 같은 batch_size 사용 권장\n",
    "batch_size = 64\n",
    "image_size = 64\n",
    "\n",
    "# train할 때 저장한 모델 파라미터 pt파일 불러오기\n",
    "model = DDPMModel().to(device)\n",
    "model_path = ''\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f'Model is loaded')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    x0 = sample_ddim(model,\n",
    "                     shape=(batch_size, 3, image_size, image_size),\n",
    "                     alphas_cumprod=alphas_cumprod,\n",
    "                     ddim_steps=200,\n",
    "                     device=device\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8NhOU6CALI1"
   },
   "source": [
    "# Accuracy Score - Lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lvrMkdft8sAq"
   },
   "outputs": [],
   "source": [
    "lpips_fn = lpips.LPIPS(net='alex').to(device)\n",
    "lpips_fn.eval()\n",
    "\n",
    "N = batch_size\n",
    "\n",
    "# batch_size 개수만큼의 3*64*64 원본 이미지를 리스트에 저장\n",
    "real_imgs = []\n",
    "count = 0\n",
    "for img in test_dataset:\n",
    "    real_imgs.append(img)\n",
    "    count += 1\n",
    "    if count==batch_size:\n",
    "        break\n",
    "\n",
    "# batch_size 개수만큼의 3*64*64 샘플링 이미지를 리스트에 저장\n",
    "fake_imgs = [x0[i] for i in range(N)]\n",
    "\n",
    "scores = []\n",
    "# 진짜/가짜 이미지 쌍의 점수 비교\n",
    "with torch.no_grad():\n",
    "    for i in range(N):\n",
    "        real_img = real_imgs[i]\n",
    "        fake_img = fake_imgs[i]\n",
    "        fake_img = fake_img.to(device)\n",
    "        real_img = real_img.to(device)\n",
    "        # dist shape: [1,1,1,1]\n",
    "        dist = lpips_fn(real_img, fake_img)\n",
    "        scores.append(dist.item())\n",
    "\n",
    "scores = np.array(scores)\n",
    "\n",
    "print(f\"Evaluated {N} pairs\")\n",
    "print(f\"Mean LPIPS: {scores.mean():.4f}\")\n",
    "print(f\"Median LPIPS: {np.median(scores):.4f}\")\n",
    "print(f\"Std LPIPS: {scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6iZdUQleASNO"
   },
   "source": [
    "# Accuracy Score - FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t3G8r9gg95GN"
   },
   "outputs": [],
   "source": [
    "def tensor_batch_to_folder(imgs, folder):\n",
    "    # imgs에 들어있는 이미지들을 폴더에 저장합니다.\n",
    "    # 저장된 폴더의 디렉토리를 이후 함수에 전달해 FID 점수를 계산합니다.\n",
    "\n",
    "    # 폴더가 이미 있으면 비우고, 없으면 생성\n",
    "    if os.path.isdir(folder):\n",
    "        shutil.rmtree(folder)\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    # 이미지 변환\n",
    "    imgs = imgs.detach().cpu()\n",
    "    imgs_uint8 = ((imgs + 1.0) * 0.5 * 255.0).clamp(0, 255).to(torch.uint8)\n",
    "\n",
    "    # 각 이미지 폴더에 저장\n",
    "    B = imgs_uint8.size(0)\n",
    "    for i in range(B):\n",
    "        img_i = imgs_uint8[i]\n",
    "        np_img = img_i.permute(1, 2, 0).numpy()\n",
    "        # PIL 이미지로 변환\n",
    "        pil_img = Image.fromarray(np_img)\n",
    "        # 파일명: img_000.png, img_001.png, ...\n",
    "        filename = os.path.join(folder, f\"img_{i:04d}.png\")\n",
    "        pil_img.save(filename)\n",
    "\n",
    "def calculate_fid_from_tensors(real_imgs, fake_imgs, batch_size=64, device='cuda'):\n",
    "    # 입력으로 받은 진짜/가짜 이미지들을 폴더에 저장한 후에\n",
    "    # fid_score 함수에 폴더의 디렉토리를 입력으로 넣어 FID 점수를 얻습니다.\n",
    "\n",
    "    # 임시 디렉터리 생성 (real/fake)\n",
    "    temp_dir = tempfile.mkdtemp(prefix=\"fid_temp_\")\n",
    "    dir_real = Path(temp_dir) / \"real\"\n",
    "    dir_fake = Path(temp_dir) / \"fake\"\n",
    "\n",
    "    # 텐서를 각각 이미지로 저장\n",
    "    tensor_batch_to_folder(real_imgs, str(dir_real))\n",
    "    tensor_batch_to_folder(fake_imgs, str(dir_fake))\n",
    "\n",
    "    # pytorch-fid를 이용해 FID 계산\n",
    "    fid_value = fid_score.calculate_fid_given_paths(\n",
    "        [str(dir_real), str(dir_fake)],\n",
    "        batch_size=batch_size,\n",
    "        device=device,\n",
    "        dims=2048,\n",
    "    )\n",
    "\n",
    "    # 임시 디렉터리 삭제\n",
    "    # shutil.rmtree(temp_dir, ignore_errors=True)\n",
    "\n",
    "    print(temp_dir)\n",
    "\n",
    "    return float(fid_value)\n",
    "\n",
    "# real, fake 모두 B*3*64*64 크기의 텐서입니다.\n",
    "for img in test_loader:\n",
    "    real = img\n",
    "    break\n",
    "\n",
    "fake = x0\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "fid = calculate_fid_from_tensors(\n",
    "    real, fake, batch_size=batch_size, device=device\n",
    ")\n",
    "print(f\"FID score: {fid:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PPNvkYUmLQVC"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_sample(x0, idx=0):\n",
    "    img = x0[idx]\n",
    "    img = np.transpose(img, (1, 2, 0))\n",
    "    img = (img + 1.0) / 2.0\n",
    "    img = np.clip(img, 0, 1)\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Sampled x_0\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gQ0IxditLSZR"
   },
   "outputs": [],
   "source": [
    "visualize_sample(x0,idx=0)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Ev1z4SkiWdAT",
    "d5xNoEOZDYwy",
    "JlSYB0-wDf38",
    "Pe5Inl-h_xmu"
   ],
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
