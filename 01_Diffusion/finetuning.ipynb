{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["IdjoWZlRUT_m","rpptkxiYBara","FbtbB53SBck-","Wnawb74LVqfx","rOzA7tMPK780"],"gpuType":"T4","authorship_tag":"ABX9TyPiabw3vSp/Dg+AKjgURhW3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Setting"],"metadata":{"id":"IdjoWZlRUT_m"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"qdlmsZo0BKx1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# FFHQ 압축파일 Colab 런타임 VM으로 복사\n","# 디렉토리에 ffhq.zip 파일이 있어야 합니다.\n","!cp /content/sample_directory/ffhq.zip /content/\n","\n","# VM에 파일 압축풀기\n","# /content/data 경로에 모든 이미지가 저장됩니다.\n","!unzip -q /content/ffhq.zip -d /content/data/"],"metadata":{"id":"OM5_i4EwVdEF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# fake_imgs 압축파일 Colab 런타임 VM으로 복사\n","# 디렉토리에 fake_imgs.zip 파일이 있어야 합니다.\n","# fake_imgs은 DDIM 모델로 샘플링한 가짜 이미지들입니다.\n","!cp /content/sample_directory/fake_imgs.zip /content/\n","!mkdir /content/data/fake\n","\n","# VM에 파일 압축풀기\n","# /content/data/fake 경로에 모든 이미지가 저장됩니다.\n","!unzip -q /content/fake_imgs.zip -d /content/data/fake/"],"metadata":{"id":"g13j5ID_WrYC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import os\n","from PIL import Image\n","from tqdm.auto import tqdm\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.optim import Adam\n","from torch.utils.data import Dataset, DataLoader\n","from torch.nn.utils import clip_grad_norm_\n","from torchvision import transforms, datasets, utils"],"metadata":{"id":"_99_NRY9BHYT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Dataset"],"metadata":{"id":"NIzwp3MiVOmx"}},{"cell_type":"code","source":["class FFHQDataset(Dataset):\n","    def __init__(self, num_total_img, real_img_paths, fake_img_paths, transform=None):\n","        num_real_imgs = num_total_img // 2\n","        num_fake_imgs = num_total_img - num_real_imgs\n","        self.num_img = num_total_img\n","        self.real_img_paths = real_img_paths\n","        self.fake_img_paths = fake_img_paths\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.real_img_paths + self.fake_img_paths)\n","\n","    def __getitem__(self, idx):\n","        if idx < len(self.real_img_paths):\n","            path = self.real_img_paths[idx]\n","            label = 1\n","        else:\n","            path = self.fake_img_paths[idx - len(self.real_img_paths)]\n","            label = 0\n","\n","        img = Image.open(path).convert('RGB')\n","        if self.transform:\n","            img = self.transform(img)\n","\n","        label = torch.tensor(label, dtype=torch.float32)\n","\n","        return img, label\n","\n","real_img_dir = '/content/data/thumbnails128x128'\n","fake_img_dir = '/content/data/fake'\n","\n","temp_real = os.listdir(real_img_dir)\n","temp_fake = os.listdir(fake_img_dir)\n","\n","# train/test 이미지 개수 => 진짜 이미지 + 가짜 이미지\n","num_train_image = 6000\n","num_test_image = 200\n","\n","train_real = temp_real[:num_train_image//2]\n","train_fake = temp_fake[:num_train_image//2]\n","test_real = temp_real[num_train_image//2:num_train_image//2+num_test_image//2]\n","test_fake = temp_fake[num_train_image//2:num_train_image//2+num_test_image//2]\n","\n","train_real_paths = [os.path.join(real_img_dir, i) for i in train_real]\n","train_fake_paths = [os.path.join(fake_img_dir, i) for i in train_fake]\n","test_real_paths = [os.path.join(real_img_dir, i) for i in test_real]\n","test_fake_paths = [os.path.join(fake_img_dir, i) for i in test_fake]\n","\n","train_dataset = FFHQDataset(num_total_img=num_train_image, real_img_paths=train_real_paths, fake_img_paths=train_fake_paths, transform=train_transform)\n","test_dataset = FFHQDataset(num_total_img=num_test_image, real_img_paths=test_real_paths, fake_img_paths=test_fake_paths, transform=test_transform)\n","\n","print(f\"Train dataset size: {len(train_dataset)}\")\n","print(f\"Test dataset size: {len(test_dataset)}\")\n","\n","train_transform = transforms.Compose([\n","    transforms.Resize((64, 64)),\n","    transforms.RandomHorizontalFlip(p=0.5),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.5]*3, [0.5]*3)\n","])\n","test_transform = transforms.Compose([\n","    transforms.Resize((64, 64)),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.5]*3, [0.5]*3)\n","])\n","\n","batch_size = 128\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)"],"metadata":{"id":"-pIY-MsqVMUQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model"],"metadata":{"id":"rpptkxiYBara"}},{"cell_type":"code","source":["# ProGAN에서 구현한 discriminator 그대로 사용하시면 됩니다."],"metadata":{"id":"wHkMhAZFBHSE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Train setup"],"metadata":{"id":"FbtbB53SBck-"}},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","loss_func = nn.BCELoss()\n","model = Discriminator(steps=4).to(device)\n","\n","# 학습된 discriminator 가중치입니다.\n","checkpoint_path = 'model.pt'\n","checkpoint = torch.load(checkpoint_path)\n","model.load_state_dict(checkpoint)\n","\n","# Discriminator에서 가중치 업데이트를 하지 않을 (동결할) 레이어 수\n","n_freeze = 4\n","\n","for i in range(n_freeze):\n","    # fromRGB 레이어 i 동결\n","    for p in model.fromrgb_layers[i].parameters():\n","        p.requires_grad = False\n","    # prog_block i 동결\n","    for p in model.prog_blocks[i].parameters():\n","        p.requires_grad = False\n","\n","# 옵티마이저에는 requires_grad=True 인 파라미터만 넣어줍니다.\n","optimizer = torch.optim.Adam(\n","    filter(lambda p: p.requires_grad, model.parameters()),\n","    lr=5e-5\n",")"],"metadata":{"id":"CtK0RWmRBHNN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_params = sum(p.numel() for p in model.parameters())\n","trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print(f\"Trainable: {trainable_params}/{total_params} 파라미터\")"],"metadata":{"id":"oxLbaEJvFBXn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Test before Train"],"metadata":{"id":"Wnawb74LVqfx"}},{"cell_type":"code","source":["model.eval()\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for image,label in test_loader:\n","        image = image.to(device)\n","        label = label.to(device)\n","\n","        output = model(image, 1.0).view(-1)\n","        predicted = (output.data > 0.5).long().view(-1)\n","        total += label.size(0)\n","        correct += (predicted == label.long()).sum().item()\n","\n","    accuracy = 100 * correct / total\n","    print(f'Accuracy: {accuracy:.2f}%')"],"metadata":{"id":"S0On_H71VpZN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Train"],"metadata":{"id":"rOzA7tMPK780"}},{"cell_type":"code","source":["for epoch in tqdm(range(10)):\n","    model.train()\n","    for image,label in train_loader:\n","        image = image.to(device)\n","        label = label.to(device)\n","\n","        output = model(image, 1.0).view(-1)\n","        loss = loss_func(output, label)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        clip_grad_norm_(model.parameters(), 1.0)\n","        optimizer.step()\n","\n","    model.eval()\n","    with torch.no_grad():\n","        correct = 0\n","        total = 0\n","        for image,label in test_loader:\n","            image = image.to(device)\n","            label = label.to(device)\n","\n","            output = model(image, 1.0).view(-1)\n","            predicted = (output.data > 0.5).long().view(-1)\n","            total += label.size(0)\n","            correct += (predicted == label.long()).sum().item()\n","\n","        accuracy = 100 * correct / total\n","        print(f'Epoch {epoch}, Loss: {loss.item():.4f}, Accuracy: {accuracy:.2f}%')\n","\n","    # 모델 가중치 저장\n","    if epoch % 5==0:\n","        torch.save(model.state_dict(), f'/model_{epoch}.pt')"],"metadata":{"id":"eYts70bEBHKG"},"execution_count":null,"outputs":[]}]}