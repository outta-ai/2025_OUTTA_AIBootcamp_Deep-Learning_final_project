{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVVxa1R7Ry1n"
   },
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CP02ch0LxaZE"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ui1VQvZDQsQ-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torchvision import transforms\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8VmMHZBqQsNb"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sb4BTjH4R3hZ"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rEVmSEQFQYPx"
   },
   "outputs": [],
   "source": [
    "def swish(x):\n",
    "    # DDPM에서 쓰이는 활성화함수\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "def get_timestep_embedding(t,channel):\n",
    "    # DDPM은 타임스텝 t 또한 입력으로 받아 이미지에 낀 노이즈의 값을 예측합니다.\n",
    "    # get_timestep_embedding 함수는 batch_size만큼의 타임스텝 t를 입력으로 받아서,\n",
    "    # 이에 대응되는 embedding vector(torch.Size([B,channel]))를 반환합니다.\n",
    "    # Transformer의 sinusodial positional encoding과 동일합니다. (자세한 수식은 PPT 참고해주세요.)\n",
    "\n",
    "    # channel 값은 짝수 int로 입력해야 코드가 잘 작동합니다.\n",
    "    half = channel // 2\n",
    "    device = t.device\n",
    "\n",
    "    # (구현) timestep embedding\n",
    "    inv_freq = torch.exp(-math.log(10000) * torch.arange(half)/half).to(device)\n",
    "    args = t.float().unsqueeze(1) * inv_freq.unsqueeze(0)\n",
    "    emb = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)\n",
    "\n",
    "    return emb\n",
    "\n",
    "class GroupNorm(nn.GroupNorm):\n",
    "    # Group normalization\n",
    "    def __init__(self, num_channels, num_groups=8, eps=1e-6):\n",
    "        super().__init__(num_groups, num_channels, eps=eps)\n",
    "\n",
    "def conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True, init_scale=1.0):\n",
    "    # 3x3 convolution\n",
    "    conv = nn.Conv2d(in_ch, out_ch, kernel_size, stride, padding, bias=bias)\n",
    "    with torch.no_grad():\n",
    "        # init_scale==0으로 설정시 처음에 레이어의 가중치가 0으로 초기화됩니다.\n",
    "        conv.weight.data *= init_scale\n",
    "    return conv\n",
    "\n",
    "def nin(in_ch, out_ch, init_scale=1.0):\n",
    "    # 1x1 convolution\n",
    "    layer = nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=1, padding=0)\n",
    "    with torch.no_grad():\n",
    "        layer.weight.data *= init_scale\n",
    "    return layer\n",
    "\n",
    "def linear(in_features, out_features, init_scale=1.0):\n",
    "    # linear layer\n",
    "    fc = nn.Linear(in_features, out_features)\n",
    "    with torch.no_grad():\n",
    "        fc.weight.data *= init_scale\n",
    "    return fc\n",
    "\n",
    "class DownsampleBlock(nn.Module):\n",
    "    # B*C*2H*2W 이미지를 입력으로 받아, B*C*H*W 이미지를 반환합니다.\n",
    "    # with_conv 여부에 따라 downsampling 방식이 달라집니다.\n",
    "    def __init__(self, channels, with_conv=True):\n",
    "        super().__init__()\n",
    "        if with_conv:\n",
    "            self.op = nn.Conv2d(channels, channels, kernel_size=3, stride=2, padding=1)\n",
    "        else:\n",
    "            self.op = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.op(x)\n",
    "\n",
    "class UpsampleBlock(nn.Module):\n",
    "    # B*C*H*W 이미지를 입력으로 받아, B*C*2H*2W 이미지를 반환합니다.\n",
    "    # with_conv 여부에 따라 upsampling 방식이 달라집니다.\n",
    "    def __init__(self, channels, with_conv=True):\n",
    "        super().__init__()\n",
    "        self.with_conv = with_conv\n",
    "        if with_conv:\n",
    "            self.conv = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor=2.0, mode='nearest')\n",
    "        if self.with_conv:\n",
    "            x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    # DDPM의 주요 구성요소 중 하나인 ResnetBlock입니다.\n",
    "    # 모델 구조 및 구현에 관한 세부사항은 PPT 참고바랍니다.\n",
    "    # 모든 레이어의 init_scale은 1.0으로 설정합니다.\n",
    "    def __init__(self, in_channels, out_channels, temb_channels=256, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.temb_channels = temb_channels # channel of timestep embedding\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # (구현) ResnetBlock\n",
    "        self.norm1 = GroupNorm(self.in_channels)\n",
    "        self.conv1 = conv2d(self.in_channels, self.out_channels,\n",
    "                            kernel_size=3, stride=1, padding=1, init_scale=1.0)\n",
    "        self.temb_proj = linear(self.temb_channels, self.out_channels, init_scale=1.0)\n",
    "        self.norm2 = GroupNorm(self.out_channels)\n",
    "        self.conv2 = conv2d(self.out_channels, self.out_channels,\n",
    "                            kernel_size=3, stride=1, padding=1, init_scale=1.0)\n",
    "        self.conv_shortcut = nin(self.in_channels, self.out_channels)\n",
    "\n",
    "    def forward(self, x, temb):\n",
    "        # B*in_channels*H*W 크기의 텐서 x와 B*temb_channels 크기의 temb를 입력으로 받아서\n",
    "        # B*out_channels*H*W 크기의 텐서를 반환합니다.\n",
    "\n",
    "        # (구현) ResnetBlock\n",
    "        h = self.norm1(x)\n",
    "        h = swish(h)\n",
    "        h = self.conv1(h)\n",
    "        h_temb = swish(temb)\n",
    "        h_temb = self.temb_proj(h_temb)\n",
    "        h_temb = h_temb[:, :, None, None]\n",
    "        h = h + h_temb\n",
    "        h = self.norm2(h)\n",
    "        h = swish(h)\n",
    "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        h = self.conv2(h)\n",
    "        x = self.conv_shortcut(x)\n",
    "        return x + h\n",
    "\n",
    "class AttnBlock(nn.Module):\n",
    "    # DDPM의 주요 구성요소 중 하나인 ResnetBlock입니다.\n",
    "    # 모델 구조 및 구현에 관한 세부사항은 PPT 참고바랍니다.\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.norm = GroupNorm(channels)\n",
    "        self.q = nin(channels, channels)\n",
    "        self.k = nin(channels, channels)\n",
    "        self.v = nin(channels, channels)\n",
    "        self.proj_out = nin(channels, channels, init_scale=0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        h = self.norm(x)\n",
    "\n",
    "        # Q,K,V 텐서 생성\n",
    "        q = self.q(h)\n",
    "        k = self.k(h)\n",
    "        v = self.v(h)\n",
    "\n",
    "        q = q.view(B, C, H * W).permute(0, 2, 1)  # (B, T, C)\n",
    "        k = k.view(B, C, H * W).permute(0, 2, 1)  # (B, T, C)\n",
    "        v = v.view(B, C, H * W).permute(0, 2, 1)  # (B, T, C)\n",
    "\n",
    "        scale = q.shape[-1] ** -0.5\n",
    "\n",
    "        attn = torch.softmax(torch.bmm(q, k.transpose(1, 2)) * scale, dim=-1)\n",
    "        h_ = torch.bmm(attn, v)\n",
    "        h_ = h_.permute(0, 2, 1).reshape(B, C, H, W)\n",
    "\n",
    "        h_ = self.proj_out(h_) # torch.Size([B,C,H,W])\n",
    "        return x + h_\n",
    "\n",
    "class DDPMModel(nn.Module):\n",
    "    # 최종 DDPM 모델입니다.\n",
    "    # Downsample, Middle, Upsample 블락으로 구성됩니다.\n",
    "    # 각 블락은 resnetblock, attentionblock으로 구성됩니다.\n",
    "    # 모델 구조 및 구현에 관한 세부사항은 PPT 참고바랍니다.\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=3,\n",
    "        out_channels=3,\n",
    "        ch=64,\n",
    "        ch_mult=(1,2,4),\n",
    "        num_res_blocks=2,\n",
    "        attn_resolutions={32},\n",
    "        dropout=0.0,\n",
    "        resamp_with_conv=False,\n",
    "        init_resolution=64\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.ch = ch\n",
    "        self.ch_mult = ch_mult\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.attn_resolutions = attn_resolutions\n",
    "        self.dropout = dropout\n",
    "        self.num_levels = len(ch_mult)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.resamp_with_conv = resamp_with_conv\n",
    "        self.init_resolution = init_resolution\n",
    "\n",
    "        # Timestep embedding channel\n",
    "        self.temb_ch = ch * 4\n",
    "\n",
    "        # Timestep embedding layers\n",
    "        self.temb_dense0 = linear(self.ch, self.temb_ch)\n",
    "        self.temb_dense1 = linear(self.temb_ch, self.temb_ch)\n",
    "\n",
    "        # Input conv\n",
    "        self.conv_in = conv2d(in_channels, ch, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # (구현) Downsample blocks\n",
    "        self.down_blocks = nn.ModuleList()\n",
    "\n",
    "        curr_ch = ch\n",
    "        curr_res = init_resolution\n",
    "        for level in range(self.num_levels):\n",
    "            level_blocks = nn.ModuleList()\n",
    "            out_ch = ch * ch_mult[level]\n",
    "            for i in range(num_res_blocks):\n",
    "                level_blocks.append(ResnetBlock(curr_ch, out_ch, temb_channels=self.temb_ch, dropout=dropout))\n",
    "                if curr_res in attn_resolutions:\n",
    "                    level_blocks.append(AttnBlock(out_ch))\n",
    "                curr_ch = out_ch\n",
    "            self.down_blocks.append(level_blocks)\n",
    "            if level != self.num_levels - 1:\n",
    "                self.down_blocks.append(DownsampleBlock(curr_ch, with_conv=resamp_with_conv))\n",
    "                curr_res //= 2\n",
    "\n",
    "        # (구현) Middle blocks\n",
    "        self.mid_block = nn.ModuleList()\n",
    "\n",
    "        self.mid_block = nn.ModuleList([\n",
    "            ResnetBlock(curr_ch, curr_ch, temb_channels=self.temb_ch, dropout=dropout),\n",
    "            AttnBlock(curr_ch),\n",
    "            ResnetBlock(curr_ch, curr_ch, temb_channels=self.temb_ch, dropout=dropout)\n",
    "        ])\n",
    "\n",
    "        # (구현) Upsample blocks\n",
    "        self.up_blocks = nn.ModuleList()\n",
    "\n",
    "        self.up_blocks = nn.ModuleList()\n",
    "        for level in reversed(range(self.num_levels)):\n",
    "            level_blocks = nn.ModuleList()\n",
    "            out_ch = ch * ch_mult[level]\n",
    "            level_blocks.append(ResnetBlock(curr_ch + out_ch, out_ch, temb_channels=self.temb_ch, dropout=dropout))\n",
    "            if (init_resolution // (2 ** level)) in attn_resolutions:\n",
    "                level_blocks.append(AttnBlock(out_ch))\n",
    "            curr_ch = out_ch\n",
    "            for i in range(num_res_blocks):\n",
    "                level_blocks.append(ResnetBlock(curr_ch, curr_ch, temb_channels=self.temb_ch, dropout=dropout))\n",
    "                if (init_resolution // (2 ** level)) in attn_resolutions:\n",
    "                    level_blocks.append(AttnBlock(curr_ch))\n",
    "            if level != 0:\n",
    "                level_blocks.append(UpsampleBlock(curr_ch, with_conv=resamp_with_conv))\n",
    "            self.up_blocks.append(level_blocks)\n",
    "\n",
    "        # Output conv\n",
    "        self.norm_out = GroupNorm(curr_ch)\n",
    "        self.conv_out = conv2d(curr_ch, out_channels, kernel_size=3, stride=1, padding=1, init_scale=0.0)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Timestep embedding\n",
    "        temb = get_timestep_embedding(t, self.ch) # B*ch\n",
    "        temb = self.temb_dense0(temb) # B*4ch\n",
    "        temb = swish(temb)\n",
    "        temb = self.temb_dense1(temb) # B*4ch\n",
    "\n",
    "        skips = []\n",
    "        h = self.conv_in(x)\n",
    "\n",
    "        # (구현) down_blocks에 있는 레이어들을 따라 downsampling 진행\n",
    "        # 각 레이어의 output을 skips 리스트에 저장합니다. (이는 Upsample시 활용됩니다)\n",
    "        # resnetblock과 attentionblock에 들어가는 input이 다름을 주의해주세요.\n",
    "        down_iter = iter(self.down_blocks)\n",
    "        for level in range(self.num_levels):\n",
    "            blocks = next(down_iter)\n",
    "            for layer in blocks:\n",
    "                h = layer(h, temb) if isinstance(layer, ResnetBlock) else layer(h)\n",
    "            skips.append(h)\n",
    "            if level != self.num_levels - 1:\n",
    "                downsample = next(down_iter)\n",
    "                h = downsample(h)\n",
    "\n",
    "        # (구현) mid_blocks에 있는 레이어들을 따라 진행\n",
    "        for layer in self.mid_block:\n",
    "            h = layer(h, temb) if isinstance(layer, ResnetBlock) else layer(h)\n",
    "\n",
    "        # (구현) up_blocks에 있는 레이어들을 따라 upsampling 진행\n",
    "        # downsample 과정에서 구한 텐서와 병합 후 레이어에 넣어줍니다.\n",
    "        for level in range(self.num_levels):\n",
    "            blocks = self.up_blocks[level]\n",
    "            skip = skips.pop()\n",
    "            h = torch.cat([h, skip], dim=1)\n",
    "            h = blocks[0](h, temb)\n",
    "            for layer in blocks[1:]:\n",
    "                if isinstance(layer, ResnetBlock):\n",
    "                    h = layer(h, temb)\n",
    "                else:\n",
    "                    h = layer(h)\n",
    "\n",
    "        # Output\n",
    "        h = self.norm_out(h)\n",
    "        h = swish(h)\n",
    "        h = self.conv_out(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jojqYLe2R9oB"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gRTmW_4kQYNB"
   },
   "outputs": [],
   "source": [
    "# CelebA 압축파일 Colab 런타임 VM으로 복사\n",
    "# 디렉토리에 celeba_dataset.zip 파일이 있어야 합니다.\n",
    "!cp /content/sample_directory/celeba_dataset.zip /content/\n",
    "\n",
    "# VM에 파일 압축풀기\n",
    "# /content/data 경로에 모든 이미지가 저장됩니다.\n",
    "!unzip -q /content/celeba_dataset.zip -d /content/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ygrwMUVQYKc"
   },
   "outputs": [],
   "source": [
    "class CelebADataset(Dataset):\n",
    "    # CelebA 데이터셋 구성\n",
    "    def __init__(self, img_dir, img_num_list, transform=None):\n",
    "        # img_num_list에 있는 번호의 이미지들만으로 구성\n",
    "        self.img_paths = [os.path.join(img_dir, f'{i:06d}.jpg') for i in img_num_list]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.img_paths[idx]\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HyCzh4_-rZ2q"
   },
   "outputs": [],
   "source": [
    "# train 및 validation에 쓰이는 이미지 개수는 30000개로 제한합니다.\n",
    "num_data = 30000\n",
    "train_test_ratio = 0.9\n",
    "batch_size = 64\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize([64,64]),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize([64,64]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "])\n",
    "\n",
    "train_img_num = np.arange(1,num_data+1)[:int(num_data*train_test_ratio)]\n",
    "test_img_num = np.arange(1,num_data+1)[int(num_data*train_test_ratio):]\n",
    "\n",
    "train_dataset = CelebADataset('/content/data/img_align_celeba', img_num_list=train_img_num, transform=train_transform)\n",
    "test_dataset = CelebADataset('/content/data/img_align_celeba', img_num_list=test_img_num, transform=test_transform)\n",
    "print(f'Train dataset size: {len(train_dataset)}') # Ex) 27000\n",
    "print(f'Test dataset size: {len(test_dataset)}') # Ex) 3000\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "noQO4CjvSn41"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_VftcscQYHr"
   },
   "outputs": [],
   "source": [
    "def get_beta_alpha_linear(beta_start=0.0001, beta_end=0.02, num_timesteps=1000):\n",
    "    # DDPM 학습 및 샘플링에 쓰일 alpha, beta, alphas_cumprod 반환\n",
    "\n",
    "    betas = np.linspace(beta_start, beta_end, num_timesteps, dtype=np.float32)\n",
    "    betas = torch.tensor(betas)\n",
    "    alphas = 1.0 - betas\n",
    "    alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "    return betas, alphas, alphas_cumprod\n",
    "\n",
    "def q_sample(x0, t, noise, alphas_cumprod):\n",
    "    # 원본 이미지 x0를 입력으로 받아 t스텝만큼 diffusion process 진행\n",
    "\n",
    "    alpha_bar = alphas_cumprod[t-1].to(x0.device) # torch.Size([batch_size])\n",
    "    B = alpha_bar.size(0)\n",
    "    alpha_bar = alpha_bar.reshape((B,1,1,1))\n",
    "\n",
    "    # (구현) t스텝만큼의 diffusion process 구현\n",
    "    # 각 스텝마다 diffusion process를 반복문으로 구현하는 것이 아닙니다.\n",
    "    # x_t가 x0를 평균으로 하는 정규분포를 따름을 이용하시면 됩니다.\n",
    "    sqrt_alpha_bar = torch.sqrt(alpha_bar)\n",
    "    sqrt_one_minus_alpha_bar = torch.sqrt(1.0 - alpha_bar)\n",
    "    x_t = sqrt_alpha_bar * x0 + sqrt_one_minus_alpha_bar * noise\n",
    "\n",
    "    return x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tp2IbMpHzV8t"
   },
   "outputs": [],
   "source": [
    "def compute_mse_loss(model, x_t, t, eps):\n",
    "    # 모델이 예측한 이미지에 낀 노이즈의 값과, 실제 노이즈의 값의 차이를 반환\n",
    "\n",
    "    pred_eps = model(x_t, t)\n",
    "    loss = F.mse_loss(pred_eps, eps)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8qWwdFrCQYE8"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model,train_loader,alphas_cumprod,device,optimizer,use_gradient_clipping=True):\n",
    "    # epoch별 학습 코드\n",
    "    # 현재 epoch의 train loss 반환\n",
    "\n",
    "    train_loss_sum = 0.0\n",
    "    train_loss_cnt = 0\n",
    "\n",
    "    model.train()\n",
    "    for image in tqdm(train_loader, desc='Training'):\n",
    "        image = image.to(device)\n",
    "        eps = torch.randn(image.shape).to(device)\n",
    "        # torch.Size([batch_size])의 t 텐서 생성\n",
    "        t = torch.randint(1, 1001, (image.size(0),), dtype=torch.long).to(device)\n",
    "\n",
    "        # (구현) image(x0), t, eps 이용해서 t스텝만큼 노이즈 추가된 이미지 x_t 생성\n",
    "        x_t = q_sample(image, t, eps, alphas_cumprod)\n",
    "\n",
    "        # (구현) 적절한 loss 값 구현 (전에 구현한 함수 이용)\n",
    "        loss = compute_mse_loss(model, x_t, t, eps)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient_clipping 사용시 학습의 안정성이 올라갑니다.\n",
    "        if use_gradient_clipping:\n",
    "            max_grad_norm = 1.0\n",
    "            clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss_sum += loss.item()\n",
    "        train_loss_cnt += 1\n",
    "\n",
    "    train_loss = train_loss_sum / train_loss_cnt\n",
    "    return train_loss\n",
    "\n",
    "def test_epoch(model, test_loader, device):\n",
    "    # epoch별 test 코드\n",
    "    # 현재 epoch의 test loss 반환\n",
    "\n",
    "    test_loss_sum = 0.0\n",
    "    test_loss_cnt = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for image in tqdm(test_loader, desc='Evaluating'):\n",
    "            image = image.to(device)\n",
    "            eps = torch.randn(image.shape).to(device)\n",
    "            t = torch.randint(1, 1001, (image.size(0),), dtype=torch.long).to(device)\n",
    "\n",
    "            # (구현) image(x0), t, eps 이용해서 t스텝만큼 노이즈 추가된 이미지 x_t 생성\n",
    "            # train 부분과 동일\n",
    "            x_t = q_sample(image, t, eps, alphas_cumprod)\n",
    "\n",
    "            # (구현) 적절한 loss 값 구현 (전에 구현한 함수 이용)\n",
    "            # train 부분과 동일\n",
    "            loss = compute_mse_loss(model, x_t, t, eps)\n",
    "\n",
    "            test_loss_sum += loss.item()\n",
    "            test_loss_cnt += 1\n",
    "\n",
    "        test_loss = test_loss_sum / test_loss_cnt\n",
    "    return test_loss\n",
    "\n",
    "def train(model,train_loader,test_loader,alphas_cumprod,device,optimizer,num_epochs,use_gradient_clipping=True):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    for epoch in tqdm(range(1, num_epochs+1)):\n",
    "        train_loss = train_epoch(model, train_loader, alphas_cumprod, device, optimizer, use_gradient_clipping)\n",
    "        test_loss = test_epoch(model, test_loader, device)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tq-nQgmK2X4b"
   },
   "outputs": [],
   "source": [
    "# 모델 하이퍼파라미터 권장 설정값\n",
    "ch = 64\n",
    "ch_mult = (1, 2, 4)\n",
    "num_res_blocks = 2\n",
    "attn_resolutions = {32}\n",
    "dropout = 0.0\n",
    "resamp_with_conv = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J1CiOM6_QYCD"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0'if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "_, _, alphas_cumprod = get_beta_alpha_linear()\n",
    "alphas_cumprod = alphas_cumprod.to(device)\n",
    "\n",
    "model = DDPMModel(\n",
    "    ch=ch,\n",
    "    ch_mult=ch_mult,\n",
    "    num_res_blocks=num_res_blocks,\n",
    "    attn_resolutions=attn_resolutions,\n",
    "    dropout=dropout,\n",
    "    resamp_with_conv=resamp_with_conv,\n",
    "    init_resolution=64\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-3, weight_decay=1e-4)\n",
    "\n",
    "model_path = '/content/drive/MyDrive/cv프로젝트/부트캠프/_90.pth'\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = 1e-6\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_losses, test_losses = train(model, train_loader, test_loader, alphas_cumprod, device, optimizer, 10, True)\n",
    "\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}\n",
    "torch.save(checkpoint, '/content/drive/MyDrive/cv프로젝트/부트캠프/_100.pth')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "OVVxa1R7Ry1n",
    "sb4BTjH4R3hZ",
    "jojqYLe2R9oB",
    "noQO4CjvSn41"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
